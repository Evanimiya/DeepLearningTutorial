{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ENVIORNMENT\n",
    "# OS : Ubuntu 14.04.5\n",
    "# Interpreter : Python 3.6.2\n",
    "# Pytorch version : 0.2.0, gpu version.\n",
    "# CUDA : 8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Pytorch Basics\n",
    "\n",
    "Pytorch는 파이썬 기반의 딥러닝 모델 개발을 타겟으로 하는 과학 계산용 패키지 입니다. Pytorch와 같은 딥러닝 라이브러리는 대표적으로 Tensorflow, Caffe, Theano, Chainer 등이 있고, 각각 장단점이 있습니다. 이와같은 라이브러리를 사용하면, GPU의 연산력을 사용하여 빠른 연산을 수행하여 딥러닝 모델을 학습시킬 수 있습니다. Pytorch에는 다음과 같은 장단점이 있습니다.\n",
    "\n",
    "**장점**\n",
    "- Numpy 기반의 계산을 GPU의 빠른 연산을 사용하여 처리할 수 있다.\n",
    "- Tensorflow와 비교했을 떄, Pytorch는 동적 그래프 생성을 이요하여 정적 계산 그래프를 만들 필요 없이, 더 간단하고 유연하게  모델을 구성할 수 있다.\n",
    "- 활발한 커뮤니티와 포럼(https://discuss.pytorch.org)이 형성되어 있으며, 개발 활동(https://github.com/pytorch) 또한 활발하다. \n",
    "\n",
    "\n",
    "** 단점 **\n",
    "- Tensorflow 보다는 분산 학습 시스템이 좋지 않다. (하지만 Pytorch 에서도 multi-GPU 사용 가능하다.)\n",
    "- 초보자가 디버깅하기 용이하다.\n",
    "- 현재, 개발된 모델을 상용화하여 배포할 수단이 마땅치 않다, 그러므로 주로 연구 용도로 쓰인다.(Tensorflow의 경우에는 여러 배포 수단이 마련되어있다.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "import torch\n",
       "import numpy as np"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 텐서(Tensors)\n",
    "\n",
    "Pytorch에서 텐서란 고차원 데이터의 연산을 위한 데이터타입으로, Numpy의 ndarray와 기본적으로 같은 개념입니다.\n",
    "그러나 Numpy의 ndarray는 GPU 연산을 적용할 수 없다는 단점이 있습니다. (GPU 연산은 CPU 연산보다 약 50배 정도 빠른 속도를 갖습니다.)\n",
    "매우 많은 연산량을 요구하는 딥러닝 모델 학습에서는 GPU 연산이 필수적이므로, Pytorch에서는 GPU 연산에 적용 가능한 Tensor 데이터 타입을 사용합니다. 이 Tensor 데이터는 cuda 데이터타입으로 변환시키면, GPU의 고속 연산에 쓰일 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 텐서를 생성하는 여러가지 방법들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Various ways to create Tensor\n",
      "\n",
      "tensor_from_list :  \n",
      " 1  2  3\n",
      "-4 -5 -6\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "zero_tensor :  \n",
      " 0  0  0\n",
      " 0  0  0\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "empty tensor:  \n",
      " 0  0  0\n",
      " 0  0  0\n",
      "[torch.IntTensor of size 2x3]\n",
      "\n",
      "Apply .abs_():  \n",
      " 1  2  3\n",
      " 4  5  6\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "uninitialized_tensor :  \n",
      "-4.1687e+30  4.5602e-41 -4.1687e+30\n",
      " 4.5602e-41  4.4842e-44  0.0000e+00\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "random_tensor :  \n",
      " 0.1137  0.9316  0.9716\n",
      " 0.6507  0.6547  0.1322\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "normal_tensor :  \n",
      " 0.1877 -1.0770  0.5325\n",
      " 1.4852 -1.2210  0.1852\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "from_numpy_tensor :  \n",
      " 1  2  3\n",
      " 6  5  4\n",
      "[torch.LongTensor of size 2x3]\n",
      "\n",
      "from_tensor_ndarray :  [[1 2 3]\n",
      " [6 5 4]]\n"
     ]
    }
   ],
   "source": [
    "# 텐서를 만들어보자.\n",
    "\n",
    "print(\"Various ways to create Tensor\\n\")\n",
    "\n",
    "# 파이썬 리스트에서 초기화\n",
    "tensor_from_list = torch.FloatTensor([[1,2,3],[-4,-5,-6]])\n",
    "print(\"tensor_from_list : \",tensor_from_list)\n",
    "\n",
    "# 비어있는 텐서 1\n",
    "zero_tensor = torch.zeros(2,3)\n",
    "print(\"zero_tensor : \",zero_tensor)\n",
    "\n",
    "# 비어있는 텐서 2\n",
    "empty_tensor = torch.IntTensor(2,3).zero_()\n",
    "print(\"empty tensor: \", empty_tensor)\n",
    "\n",
    "# 끝에 _(under score)가 붙은 method는 텐서 자체를 변화(mutate)시킨다.\n",
    "tensor_from_list.abs_()\n",
    "print(\"Apply .abs_(): \", tensor_from_list)\n",
    "\n",
    "# 초기값이 주어지지 않은 텐서는 임의로 초기화된다.\n",
    "uninitialized_tensor = torch.Tensor(2,3)\n",
    "print(\"uninitialized_tensor : \",uninitialized_tensor)\n",
    "\n",
    "# [0,1) uniform distribution에서 초기화\n",
    "random_tensor = torch.rand(2,3)\n",
    "print(\"random_tensor : \",random_tensor)\n",
    "\n",
    "# N(0,1) Normal distribution에서 초기화\n",
    "normal_tensor = torch.randn(2,3)\n",
    "print(\"normal_tensor : \",normal_tensor)\n",
    "\n",
    "# ndarray에서 텐서 생성\n",
    "ndarr = np.array([[1,2,3],[6,5,4]])\n",
    "from_numpy_tensor = torch.from_numpy(ndarr)\n",
    "print(\"from_numpy_tensor : \", from_numpy_tensor)\n",
    "\n",
    "# Tensor에서 ndarray 생성\n",
    "from_tensor_ndarray = from_numpy_tensor.numpy()\n",
    "print(\"from_tensor_ndarray : \", from_tensor_ndarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이외에도, 여러가지 방법으로 텐서를 생성해 낼 수 있습니다.\n",
    "\n",
    "(참고 : http://pytorch.org/docs/master/torch.html?highlight=randn#creation-ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 텐서 indexing, slicing\n",
    "\n",
    "numpy의 ndarray을 변화시키는 여러 함수가 정의되어 있듯, Pytorch 에서도 텐서를 변화시키는 다양한 함수가 정의되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original :  \n",
      " 2.0982  1.6688  0.5680  1.2038 -0.6548\n",
      " 0.4918  1.5725  1.1214  0.5146 -0.9568\n",
      "-0.8925  1.5325  1.0058  0.2413 -1.5421\n",
      "[torch.FloatTensor of size 3x5]\n",
      "\n",
      "Concat through axis 0 : \n",
      " 2.0982  1.6688  0.5680  1.2038 -0.6548\n",
      " 0.4918  1.5725  1.1214  0.5146 -0.9568\n",
      "-0.8925  1.5325  1.0058  0.2413 -1.5421\n",
      " 2.0982  1.6688  0.5680  1.2038 -0.6548\n",
      " 0.4918  1.5725  1.1214  0.5146 -0.9568\n",
      "-0.8925  1.5325  1.0058  0.2413 -1.5421\n",
      " 2.0982  1.6688  0.5680  1.2038 -0.6548\n",
      " 0.4918  1.5725  1.1214  0.5146 -0.9568\n",
      "-0.8925  1.5325  1.0058  0.2413 -1.5421\n",
      "[torch.FloatTensor of size 9x5]\n",
      "\n",
      "Concat through axis 1 :  \n",
      "\n",
      "Columns 0 to 9 \n",
      " 2.0982  1.6688  0.5680  1.2038 -0.6548  2.0982  1.6688  0.5680  1.2038 -0.6548\n",
      " 0.4918  1.5725  1.1214  0.5146 -0.9568  0.4918  1.5725  1.1214  0.5146 -0.9568\n",
      "-0.8925  1.5325  1.0058  0.2413 -1.5421 -0.8925  1.5325  1.0058  0.2413 -1.5421\n",
      "\n",
      "Columns 10 to 14 \n",
      " 2.0982  1.6688  0.5680  1.2038 -0.6548\n",
      " 0.4918  1.5725  1.1214  0.5146 -0.9568\n",
      "-0.8925  1.5325  1.0058  0.2413 -1.5421\n",
      "[torch.FloatTensor of size 3x15]\n",
      "\n",
      "chunk_tensor :  (\n",
      " 2.0982  1.6688  0.5680  1.2038 -0.6548\n",
      "[torch.FloatTensor of size 1x5]\n",
      ", \n",
      " 0.4918  1.5725  1.1214  0.5146 -0.9568\n",
      "[torch.FloatTensor of size 1x5]\n",
      ", \n",
      "-0.8925  1.5325  1.0058  0.2413 -1.5421\n",
      "[torch.FloatTensor of size 1x5]\n",
      ")\n",
      "nonzero_index :  \n",
      " 0  0\n",
      " 1  1\n",
      " 2  2\n",
      "[torch.LongTensor of size 3x2]\n",
      "\n",
      "trans_tensor \n",
      " 2.0982  0.4918 -0.8925\n",
      " 1.6688  1.5725  1.5325\n",
      " 0.5680  1.1214  1.0058\n",
      " 1.2038  0.5146  0.2413\n",
      "-0.6548 -0.9568 -1.5421\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 텐서를 여러가지 방식으로 변화시켜보자.\n",
    "\n",
    "X = torch.randn(3,5)\n",
    "print(\"Original : \",X)\n",
    "\n",
    "# Concatenation\n",
    "concat_tensor_0 = torch.cat((X,X,X),0)\n",
    "print(\"Concat through axis 0 :\", concat_tensor_0)\n",
    "concat_tensor_1 = torch.cat((X,X,X),1)\n",
    "print(\"Concat through axis 1 : \", concat_tensor_1)\n",
    "\n",
    "# Chunking\n",
    "chunk_tensor = torch.chunk(X,3,dim=0)\n",
    "print(\"chunk_tensor : \", chunk_tensor)\n",
    "\n",
    "# Non-zero\n",
    "eye_tensor = torch.eye(3,3)\n",
    "nonzero_index = torch.nonzero(eye_tensor)\n",
    "print(\"nonzero_index : \", nonzero_index)\n",
    "\n",
    "# Transpose\n",
    "trans_tensor = torch.t(X)\n",
    "print(\"trans_tensor\", trans_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 텐서 operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original A :  \n",
      "-1.7592 -0.4082\n",
      "-0.4311 -0.5325\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Original B :  \n",
      "-1.4949  0.0968\n",
      " 1.1076  0.5714\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Added tensor :  \n",
      "-3.2540 -0.3114\n",
      " 0.6764  0.0390\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "clamp_tensor :  \n",
      "-0.5000 -0.4082\n",
      "-0.4311 -0.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "divide_by_const_tensor :  \n",
      "-0.8796 -0.2041\n",
      "-0.2156 -0.2662\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "divide_by_tensor :  \n",
      " 1.1768 -4.2175\n",
      "-0.3893 -0.9318\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "mul_by_const_tensor :  \n",
      "-17.5916  -4.0818\n",
      " -4.3114  -5.3247\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "mul_by_tensor :  \n",
      " 2.6297 -0.0395\n",
      "-0.4775 -0.3043\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "matrix_multiplication :  \n",
      " 2.1776 -0.4035\n",
      " 0.0547 -0.3460\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "sigmoid_tensor :  \n",
      " 0.1469  0.3993\n",
      " 0.3939  0.3699\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "sum_tensor :  -3.1309470534324646\n",
      "Mean :  -0.7827367633581161 std : 0.6531836624473548\n"
     ]
    }
   ],
   "source": [
    "# 텐서에 여러가지 연산을 적용해보자.\n",
    "\n",
    "A = torch.randn(2,2)\n",
    "B = torch.randn(2,2)\n",
    "\n",
    "print(\"Original A : \", A)\n",
    "print(\"Original B : \", B)\n",
    "\n",
    "# element-wise tensor addition\n",
    "added_tensor = torch.add(A,B)\n",
    "print(\"Added tensor : \",added_tensor)\n",
    "\n",
    "# Clamping tensor\n",
    "clamp_tensor = torch.clamp(A, min=-0.5, max=0.5)\n",
    "print(\"clamp_tensor : \", clamp_tensor)\n",
    "\n",
    "# Divide\n",
    "divide_by_const_tensor = torch.div(A,2)\n",
    "divide_by_tensor = torch.div(A,B)\n",
    "print(\"divide_by_const_tensor : \",divide_by_const_tensor)\n",
    "print(\"divide_by_tensor : \",divide_by_tensor)\n",
    "\n",
    "# Element-wise multiplication\n",
    "mul_by_const_tensor = torch.mul(A,10)\n",
    "mul_by_tensor = torch.mul(A,B)\n",
    "print(\"mul_by_const_tensor : \",mul_by_const_tensor)\n",
    "print(\"mul_by_tensor : \",mul_by_tensor)\n",
    "\n",
    "# Matrix multiplication\n",
    "matrix_mul_tensor = torch.mm(A,B)\n",
    "print(\"matrix_multiplication : \", matrix_mul_tensor)\n",
    "\n",
    "# Sigmoid\n",
    "sigmoid_tensor = torch.sigmoid(A)\n",
    "print(\"sigmoid_tensor : \",sigmoid_tensor)\n",
    "\n",
    "# Summation\n",
    "sum_tensor = torch.sum(A)\n",
    "print(\"sum_tensor : \",sum_tensor)\n",
    "\n",
    "# Mean, standard diviation\n",
    "print(\"Mean : \",torch.mean(A), \"std :\", torch.std(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이외에도 많은 텐서 연산 함수가 정의되어 있습니다. 어떤 함수가 정의되어 있는지 한 번씩 훑어본다면 나중에 도움이 될 수 있습니다. \n",
    "\n",
    "(참고 : http://pytorch.org/docs/master/torch.html?highlight=randn#math-operations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.\n",
    "\n",
    "임의의 행렬 $$ \\\n",
    "\n",
    "$$c = \\sqrt{a^2 + b^2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
